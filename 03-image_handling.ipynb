{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e3b2f880",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Image Handling\"\n",
    "teaching: 60\n",
    "exercises: 60\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9265d2",
   "metadata": {},
   "source": [
    "[**Download Chapter pdf**](03-image_handling.md.pdf)\n",
    "\n",
    "[**Download Chapter notebook (ipynb)**](03-image_handling_2.ipynb)\n",
    "\n",
    "[<span style=\"color: rgb(255, 0, 0);\">**Mandatory Lesson Feedback Survey**</span>](https://docs.google.com/forms/d/e/1FAIpQLSdr0capF7jloJhPH3Pki1B3LZoKOG16poOpuVJ7SL2LkwLHQA/viewform?pli=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17debf",
   "metadata": {},
   "source": [
    "- How to read and process images in Python?\n",
    "- How is an image mask created?\n",
    "- What are colour channels in images?\n",
    "- Why dealing with big images is tricky?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b0a31a",
   "metadata": {},
   "source": [
    "- Understanding 2-dimensional greyscale images.\n",
    "- Learning image masking.\n",
    "- 2-dimensional colour images, colour channels\n",
    "- Decreasing memory load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed736a30",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Numpy arrays\n",
    "- Plots and subplots with matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c97bc4",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "<p style='text-align: justify;'>\n",
    "This lesson has no explicit exercises. At each step, use images of your own choice to practice. There are many image file formats, different colour schemes etc for which you can try to find similar or analogous solutions.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832645d",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "### **Reading and Processing Images**\n",
    "<p style='text-align: justify;'>\n",
    "In biology, we often deal with images, for example from microscopy and different medical imaging modalities. In many cases, we wish to extract some quantitative information from these images. The focus of this lesson is to read and process images in Python. This includes:\n",
    "</p>\n",
    "\n",
    "- Working with 2-dimensional greyscale images\n",
    "- Creating and appyling binary image masks\n",
    "- Working with 2-dimensional colour images, and interpreting colour channels\n",
    "- Decreasing the memory for further processing by reducing resolution or patching\n",
    "- Working with 3-dimensional images\n",
    "\n",
    "### **Image Example**\n",
    "\n",
    "The example in [Figure 1](#ratCerebellum) is an image from the cell [image library](http://cellimagelibrary.org/home) with the following description:\n",
    "\n",
    ">\"Midsaggital section of rat cerebellum, captured using confocal imaging. Section shows inositol trisphosphate receptor (IP3R) labelled in green, DNA in blue, and synaptophysin in magenta. Honorable Mention, 2010 Olympus BioScapes Digital Imaging CompetitionÂ®.\"\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "We might want to, for example, determine the relative amounts of IP3R, DNA and synaptophysin in this image. This tutorial will guide you through some of the steps to get you started with processing images of all sorts using Python. At the end, you will have the opportunity to come back to this image example and perform some analysis of your own.\n",
    "</p>\n",
    "\n",
    "![Figure 1: Example image, rat cerebellum](fig/rat_cerebellum.jpg){#ratCerebellum}\n",
    "\n",
    "## Work Through Example\n",
    "\n",
    "### **Reading and Plotting a 2-dimensional Image**\n",
    "<p style='text-align: justify;'>\n",
    "First, we want to read in an image. For this part of the lesson, we use a histological slice through an axon bundle as an example. We use matplotlib's image module, from which we import `imread` to store the image in a variable called img. The function `imread` can interpret many different image formats, including jpg, png and tif images.\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42be131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.image import imread\n",
    "\n",
    "img = imread('fig/axon_slice.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375dcf1",
   "metadata": {},
   "source": [
    "We can check what type of variable this is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f76b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6a566",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "<class 'numpy.ndarray'>\n",
    "```\n",
    "\n",
    "This tells us that the image is stored in a Numpy array. We can check some other properties of this array, for example, what the image dimensions are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57780a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a54ef",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(2300, 3040)\n",
    "```\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "This tells us that our image is composed of 2300 by 3040 data units, or _pixels_ as we are dealing with an image. It is equivalent to the image resolution. The array has two dimensions, and so we can expect our image to be two-dimensional as well. Let us now use matplotlib.pyplot's `imshow` function to plot the image to see what it looks like. We set the colour map to `gray` to overwrite the default colour map. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import subplots, show\n",
    "\n",
    "fig, ax = subplots(figsize=(25, 15))\n",
    "\n",
    "ax.imshow(img, cmap='gray');\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa3cc6",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-4-1.png\" width=\"2400\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "`imshow` has allowed us to plot the Numpy array of our image data as a picutre. The figure is divided up into a number of pixels, and each of those pixels is assigned an intensity value stored in the Numpy array. Let's have a closer look by selecting a smaller region of our image and plotting that.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeaa671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import subplots, show\n",
    "\n",
    "fig, ax = subplots(figsize=(25, 15))\n",
    "\n",
    "ax.imshow(img[:50, :70], cmap='gray');\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583759c1",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-5-3.png\" width=\"2400\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "With `img[:50, :70]` we select the first 50 values from the first dimension, and the first 70 values from the second dimension. Thus, the image above shows a very small part of the upper left corner of our original image. As we are now zoomed in quite close to that corner, we can easily see the individual pixels here. Let's take a quick look at an even smaller section.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6878cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(25, 15))\n",
    "\n",
    "ax.imshow(img[:20, :15], cmap='gray');\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef1eed",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-6-5.png\" width=\"2400\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "This is a small section from that same upper left corner. Each square is a pixel and it has one grey value. So how exactly are the pixel values assigned? By the numbers stored in the Numpy array, `img`. Let us have a look at those values by picking a slice from the array. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d29067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img[:20, :15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d322328",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "[[ 18   9   3   4   4   1   2   7   6   8  10  12  13  14  13]\n",
    " [ 14   7   3   4   4   2   2   6   6   7   9  11  12  12  12]\n",
    " [  8   3   1   3   5   3   2   4   6   7   8   9  10  10   9]\n",
    " [  2   0   0   2   4   4   3   2   6   6   7   8   8   8   8]\n",
    " [  0   0   0   1   3   5   3   1   6   6   7   7   8   9  10]\n",
    " [  0   0   0   0   2   6   5   0   7   7   7   8   9  11  13]\n",
    " [  0   2   1   0   1   6   5   1   8   8   8  10  11  14  17]\n",
    " [  1   3   2   0   0   6   6   1   9   9   9  10  13  16  19]\n",
    " [  1   0   0   1   4   7   8   8  10  12  15  16  18  20  24]\n",
    " [  1   1   1   2   4   7   8   8  14  16  19  22  24  28  33]\n",
    " [  4   5   5   6   8  10  12  13  19  22  26  30  34  40  46]\n",
    " [  9  11  12  13  14  16  18  21  24  27  32  36  42  49  57]\n",
    " [ 12  15  17  18  19  21  25  28  32  34  38  43  49  57  65]\n",
    " [ 14  17  21  23  24  26  32  36  43  45  47  51  56  63  72]\n",
    " [ 18  23  28  31  31  35  41  47  56  57  58  60  64  70  78]\n",
    " [ 24  29  35  38  39  42  50  56  65  65  66  67  69  75  82]\n",
    " [ 32  39  40  44  51  48  52  69  77  82  83  80  81  89  94]\n",
    " [ 35  43  46  51  58  55  58  74  80  85  88  85  87  94  98]\n",
    " [ 38  50  56  61  69  66  67  80  85  90  94  95  97 102 105]\n",
    " [ 41  56  64  70  78  76  75  84  90  95 100 104 106 109 111]]\n",
    "```\n",
    "\n",
    "Each of these numbers corresponds to an intensity in the specified colourmap. These numbers range from 0 to 255, implying 256 shades of grey. \n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "We chose `cmap = gray`, which assigns darker grey colours to smaller numbers, and lighter grey colours to higher numbers. However, we can also pick a colourmap to plot our image, and we can even show a colourbar to keep track of the intensity values. Matplotlib has a large number of very nice colourmaps that you can look through [here](https://matplotlib.org/tutorials/colors/colormaps.html). We show an example of the colourmaps called `viridis` and `magma`:\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2498db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(nrows=1, ncols=2, figsize=(25, 15))\n",
    "\n",
    "p1 = ax[0].imshow(img[:20, :15], cmap='viridis')\n",
    "p2 = ax[1].imshow(img[:20, :15], cmap='magma')\n",
    "fig.colorbar(p1, ax=ax[0])\n",
    "fig.colorbar(p2, ax=ax[1]);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55aeef",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-8-7.png\" width=\"2400\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Note, that even though we can plot our greyscale image with colourful colourschemes, it still does not qualify as a colour image. This is because colour images will require __three sets__ of intensities for each pixel, not just one as in this example. In the case above, the number in the array represented a grey value and the colour was assigned to that grey value by Matplotlib. These represent 'false' colours.\n",
    "</p>\n",
    "\n",
    "### **Creating an Image Mask**\n",
    "\n",
    "Now that we know that the images are composed of a set of intensities that are just numbers in a Numpy array, we can start using these numbers to process our image. \n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "As a first approach, we can plot a histogram of the original image intensities. We use the `.flatten()` method to turn the original 2300 x 3040 array into a one-dimensional array with 6,992,000 values. This rearrangement allows the inclusion of an image as a single column in a matrix or dataframe!\n",
    "</p>\n",
    "\n",
    "The histogram plot shows how many of each of the intensities are found in this image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc08e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(10, 4))\n",
    "\n",
    "ax.hist(img.flatten(), bins = 50)\n",
    "ax.set_xlabel(\"Pixel intensity\", fontsize=16);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0bf75",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-9-9.png\" width=\"960\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "The histogram is a distribution with intensity values mostly between about 50 and 250.\n",
    "<p style='text-align: justify;'>\n",
    "The image shows a cut through an axon bundle. Say we are now interested in the myelin sheath surrounding the axons (the dark rings). We can create a __mask__ that isolates pixels whose intensity value is below a certain threshold (because darker pixels have lower intensity values). Everything below this threshold can be assigned to e.g. 1 (representing True), and everything above will be assigned to 0 (representing False). This is called a binary or Boolean mask. \n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Based on the histogram above, we might try to adjust that threshold somewhere between 100 and 200. Let's see what we get with a threshold set to 125. We first use a conditional statement to create the mask. Then we apply the mask to the image. As a result we plot both the mask and the masked image.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf9f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 125\n",
    "\n",
    "mask = img < threshold\n",
    "\n",
    "img_masked = img*mask\n",
    "\n",
    "fig, ax = subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "\n",
    "ax[0].imshow(mask, cmap='gray')\n",
    "ax[0].set_title('Binary mask', fontsize=16)\n",
    "ax[1].imshow(img_masked, cmap='gray')\n",
    "ax[1].set_title('Masked image', fontsize=16)\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de042d34",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-10-11.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "The left subplot shows the binary mask itself. White represents values where our condition is true, and black where our condition is false. The right image shows the original image after we have applied the binary mask, i.e. the original pixel intensities in regions where the mask value is true. \n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "Note that \"applying the mask\" means that the intensities where the condition is true are left unchanged and the intensities where the condition is false are multiplied with zero ans therefore set to zero.\n",
    "</p>\n",
    "Let's have a look at the resulting image histograms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf87f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
    "\n",
    "ax[0].hist(img_masked.flatten(), bins=50)\n",
    "ax[0].set_title('Histogram of masked image', fontsize=16)\n",
    "ax[0].set_xlabel(\"Pixel intensity\", fontsize=16)\n",
    "\n",
    "ax[1].hist(img_masked[img_masked != 0].flatten(), bins=25)\n",
    "ax[1].set_title('Histogram of masked image after zeros are removed', fontsize=16)\n",
    "ax[1].set_xlabel(\"Pixel intensity\", fontsize=16)\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3baa1d",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-11-13.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "On the left we show all the values for the masked image. There is a large peak at zero, as a large part of the image is masked. On the right, we show only the noon-zero pizel intensities. We can see that our mask worked as expected, only values up to 125 are found. This is because our threshold causes a sharp cut-off at a pixel intensity of 125.\n",
    "</p>\n",
    "\n",
    "### Colour Images\n",
    "<p style='text-align: justify;'>\n",
    "Often we want to work with colour images. So far, our image had a single intensity value for each pixel. In colour images, we will have three so-called channels corresponding to red, green and blue intensities. Any colour will be a composite of the intensity value for each of these colours. We now show an example with a colour image of the rat cerebellar cortex. Let us import it and check its shape.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6198b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_col = imread('fig/rat_brain_low_res.jpg')\n",
    "\n",
    "img_col.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f56743",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(929, 1000, 3)\n",
    "```\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Our image array now contains three dimensions. The first two are the spatial dimensions corresponding to the pixel positions. The last one contains the three colour channels. So we have three layers of intensity values on top of each other. \n",
    "</p>\n",
    "First, let us plot the whole image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aef730",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(25, 15))\n",
    "\n",
    "ax.imshow(img_col);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc04197",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-13-15.png\" width=\"2400\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "The sample is labeled for Hoescht stain (blue), the Inositol trisphosphate (IP3) receptor (green) and Glial fibrillary acidic protein (GFAP) (red).\n",
    "<p style='text-align: justify;'>\n",
    "Now we can visualise the three colour channels individually by slicing the Numpy array. The stack with index 0 corresponds to 'red', index 1 corresponds to 'green' and index 2 corresponds to 'blue':\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac894c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "red_channel   = img_col[:, :, 0]\n",
    "green_channel = img_col[:, :, 1]\n",
    "blue_channel  = img_col[:, :, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae341d5",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(nrows=1, ncols=3, figsize=(20, 10))\n",
    "\n",
    "imgplot_red = ax[0].imshow(red_channel, cmap=\"Reds\")\n",
    "imgplot_green = ax[1].imshow(green_channel, cmap=\"Greens\")\n",
    "imgplot_blue = ax[2].imshow(blue_channel, cmap=\"Blues\")\n",
    "\n",
    "fig.colorbar(imgplot_red, ax=ax[0], shrink=0.4)\n",
    "fig.colorbar(imgplot_green, ax=ax[1], shrink=0.4)\n",
    "fig.colorbar(imgplot_blue, ax=ax[2], shrink=0.4);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af1c79",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-15-17.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "This shows what colour combinations each of the pixels is made up of. Notice that the intensities go up to 255. This is because RGB (red, green and blue) colours are defined within the range 0-255. This gives a total of 16,777,216 possible colour combinations! \n",
    "</p>\n",
    "\n",
    "We can plot histograms of each of the colour channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "\n",
    "ax[0].hist(red_channel.flatten(), bins=50)\n",
    "ax[0].set_xlabel(\"Pixel intensity\", fontsize=16)\n",
    "ax[0].set_xlabel(\"Red channel\")\n",
    "ax[1].hist(green_channel.flatten(), bins=50)\n",
    "ax[1].set_xlabel(\"Pixel intensity\", fontsize=16)\n",
    "ax[1].set_xlabel(\"Green channel\")\n",
    "ax[2].hist(blue_channel.flatten(), bins=50)\n",
    "ax[2].set_xlabel(\"Pixel intensity\", fontsize=16)\n",
    "ax[2].set_xlabel(\"Blue channel\")\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4de4c",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-16-19.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "### **Dealing with Large Images**\n",
    "<p style='text-align: justify;'>\n",
    "Sometimes (or quite often, depending on the field of research), we have to deal with very large images that are composed of many pixels. It can be quite difficult to process these images, as they can require a lot of computer memory when they are processed. We will look at two different strategies for dealing with this problem: decreasing resolution and using patches from the original image. We will use the full-resolution version of the rat brain in the above example.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4681037",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hr = imread('fig/rat_brain.jpg')\n",
    "img_hr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3852f0a",
   "metadata": {},
   "source": [
    "```{.error}\n",
    "Error in py_call_impl(callable, dots$args, dots$keywords): DecompressionBombError: Image size (324649360 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.\n",
    "\n",
    "Detailed traceback:\n",
    "  File \"<string>\", line 1, in <module>\n",
    "  File \"/Users/sabaferdous/.virtualenvs/r-env/lib/python3.10/site-packages/matplotlib/image.py\", line 1560, in imread\n",
    "    with img_open(fname) as image:\n",
    "  File \"/Users/sabaferdous/.virtualenvs/r-env/lib/python3.10/site-packages/PIL/Image.py\", line 2994, in open\n",
    "    im = _open_core(fp, filename, prefix, formats)\n",
    "  File \"/Users/sabaferdous/.virtualenvs/r-env/lib/python3.10/site-packages/PIL/Image.py\", line 2981, in _open_core\n",
    "    _decompression_bomb_check(im.size)\n",
    "  File \"/Users/sabaferdous/.virtualenvs/r-env/lib/python3.10/site-packages/PIL/Image.py\", line 2890, in _decompression_bomb_check\n",
    "    raise DecompressionBombError(\n",
    "```\n",
    "\n",
    "```{.error}\n",
    "Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'img_hr' is not defined\n",
    "\n",
    "Detailed traceback:\n",
    "  File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "In fact, we can even get an warning from python that say something like \"Image size (324649360 pixels) exceeds limit of 244158474 pixels, could be decompression bomb DOS attack.\" This refers to malicious files which are designed to crash or cause disruption by using up a lot of memory.\n",
    "</p>\n",
    "\n",
    "We can get around this by changing the maximum pixel limit as follows. \n",
    "\n",
    "To do this, we import Image from the Python Image Library PIL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5094fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de64af94",
   "metadata": {},
   "source": [
    "Let's try again. Be patient, it might take a moment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hr = imread('fig/rat_brain.jpg')\n",
    "img_hr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510ee34",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(17360, 18701, 3)\n",
    "```\n",
    "\n",
    "Now we can plot the full high-resolution image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d9205",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(25, 15))\n",
    "\n",
    "ax.imshow(img_hr, cmap='gray');\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbcf15e",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-20-21.png\" width=\"2400\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Although now we can plot this image, it consists of over 300 million pixels, and we could run into memory problems when trying to process it. One approach is simply to reduce the resolution. One way to do this is to import the image using Image from the PIL library that we imported above. This library gives us more tools to process images, including decreasing the resolution. It is a rich library with lots of useful tools. As always, having a look at the [documentation](https://pillow.readthedocs.io/en/stable/)  and playing around is recommended!\n",
    "</p>\n",
    "\n",
    "We use `resize` to downsample the image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pil = Image.open('fig/rat_brain.jpg')\n",
    "img_small = img_pil.resize((174, 187))\n",
    "\n",
    "print(type(img_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b9efd",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "<class 'PIL.Image.Image'>\n",
    "```\n",
    "\n",
    "Plotting should now be quicker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(25, 15))\n",
    "\n",
    "ax.imshow(img_small, cmap='gray');\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de79f87",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-22-23.png\" width=\"2400\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "With this code, we have resized the image to 174 by 187 pixels. We should be aware though, that our image is no longer in a Numpy array form but rather it now has type 'PIL.Image.Image'. We can, however, easily convert it back into a Numpy array using `array`, if we wish.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65217c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "img_numpy = array(img_small)\n",
    "\n",
    "print(type(img_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1edcd",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "<class 'numpy.ndarray'>\n",
    "```\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Often, we like to have full resolution images, as resizing causes a loss of information. An alternative approach to downsampling that is commonly used is to _patch_ the images, i.e. divide the picture up into smaller chunks, or patches. \n",
    "</p>\n",
    "\n",
    "For this, we can use functionality from the [Scikit-Learn](https://scikit-learn.org/stable/) library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.image import extract_patches_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c1201f",
   "metadata": {},
   "source": [
    "'extract_patches_2d' is used to extract parts of the image. The shape of each patch as well as maxiaml number of patches can be specified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee5f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = extract_patches_2d(img_hr, (174, 187), max_patches=100)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdeef8f",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(100, 174, 187, 3)\n",
    "```\n",
    "\n",
    "Note that patching itself can be a memory-intensive task. Extracting lots and lots of patches might take a long time. To look at the patches we can use a for loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(nrows=10, ncols=10, figsize=(25, 25))\n",
    "\n",
    "ax = ax.flatten()\n",
    "\n",
    "for index in range(patches.shape[0]):\n",
    "    ax[index].imshow(patches[index, :, :, :])\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883da900",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-26-25.png\" width=\"2400\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "Now, working with these smaller, individual patches will be much more managable!\n",
    "\n",
    "### **3D Images**\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Sometimes we might want to work with 3D images. A good example for this are MRI scans. These don't come as 'csv' format but in specialised image formats. One example is `nii`, the _Neuroimaging Informatics Technology Initiative (NIfTI)_ open file format. For these types of images we will need special software. In particular, we will be using the open source library called __nibabel__. Documentation for this package is available at https://nipy.org/nibabel/. \n",
    "</p>\n",
    "\n",
    "As it is not contained in your Python installation by default, it needs to be installed first. \n",
    "\n",
    "To install it, please run: \n",
    "\n",
    "```\n",
    "conda install -c conda-forge nibabel\n",
    "```\n",
    "\n",
    "in your command line or terminal if you have an __Anaconda distribution__ of Python.\n",
    "\n",
    "Alternatively, you can install it using:\n",
    "\n",
    "```\n",
    "pip install nibabel\n",
    "```\n",
    "\n",
    "in your command line or terminal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba88b0",
   "metadata": {},
   "source": [
    "The package is now available for use. If a function comes from that packege, we call it by referring to the package using `nib`, followed by a period and the name of the function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc761ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_3d = nib.load('fig/brain.nii')\n",
    "\n",
    "img_data = img_3d.get_fdata()\n",
    "\n",
    "print(type(img_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c969a",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "<class 'numpy.memmap'>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dea254",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77afee",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(256, 256, 124)\n",
    "```\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "We can see that this image has three dimensions, and a total of 256 x 256 x 124 volume pixels (or voxels). To visualise our image, we can plot one slice at a time. Below, we show three different slices, in the transverse direction (from chin to the top of the head. To access an image from the transverse direction, you pick a single value from the third dimension of the image:\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d328c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(ncols=3, figsize=(25, 15))\n",
    "\n",
    "p1 = ax[0].imshow(img_data[:, :, 60], cmap='gray')\n",
    "p2 = ax[1].imshow(img_data[:, :, 75], cmap='gray')\n",
    "p3 = ax[2].imshow(img_data[:, :, 90], cmap='gray')\n",
    "\n",
    "fig.colorbar(p1, ax=ax[0], shrink=0.4)\n",
    "fig.colorbar(p2, ax=ax[1], shrink=0.4)\n",
    "fig.colorbar(p3, ax=ax[2], shrink=0.4);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b9071",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-29-27.png\" width=\"2400\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "These look fairly dark. We can improve the contrast, by adjusting the intensity range. This requires setting of the keyword arguments `vmin` and `vmax`. \n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "`vmin` and `vmax` define the data range that the colormap (in our case the 'grey' map) covers. By default, the colormap covers the complete value range of the supplied data. For an image that will be somewhere between 0 and 255. If we want to brighten up the darker shades of grey, we can reduce the value of `vmax`\n",
    "</p>\n",
    "\n",
    "Exanding the above code:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c234733",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(ncols=3, figsize=(25, 15))\n",
    "\n",
    "p1 = ax[0].imshow(img_data[:, :, 60], cmap='gray', vmin=0, vmax=150)\n",
    "p2 = ax[1].imshow(img_data[:, :, 75], cmap='gray', vmin=0, vmax=150)\n",
    "p3 = ax[2].imshow(img_data[:, :, 90], cmap='gray', vmin=0, vmax=150)\n",
    "\n",
    "fig.colorbar(p1, ax=ax[0], shrink=0.4)\n",
    "fig.colorbar(p2, ax=ax[1], shrink=0.4)\n",
    "fig.colorbar(p3, ax=ax[2], shrink=0.4);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86a913",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-30-29.png\" width=\"2400\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "What about the other dimensions? We can also plot coronal and sagittal slices but note that the respective slices have different pixel resolution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(nrows=3, ncols=5, figsize=(26, 18))\n",
    "\n",
    "t1 = ax[0, 0].imshow(img_data[:, :, 45].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "t2 = ax[0, 1].imshow(img_data[:, :, 60].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "t3 = ax[0, 2].imshow(img_data[:, :, 75].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "t4 = ax[0, 3].imshow(img_data[:, :, 90].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "t5 = ax[0, 4].imshow(img_data[:, :, 105].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "\n",
    "c1 = ax[1, 0].imshow(img_data[:, 50, :].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "c2 = ax[1, 1].imshow(img_data[:, 75, :].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "c3 = ax[1, 2].imshow(img_data[:, 90, :].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "c4 = ax[1, 3].imshow(img_data[:, 105, :].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "c5 = ax[1, 4].imshow(img_data[:, 120, :].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "\n",
    "s1 = ax[2, 0].imshow(img_data[75, :, :].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "s2 = ax[2, 1].imshow(img_data[90, :, :].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "s3 = ax[2, 2].imshow(img_data[105, :, :].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "s4 = ax[2, 3].imshow(img_data[120, :, :].T, cmap='gray', vmin=0, vmax=150, origin='lower')\n",
    "s5 = ax[2, 4].imshow(img_data[135, :, :].T, cmap='gray', vmin=0, vmax=150, origin='lower');\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da96b6b5",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-31-31.png\" width=\"2496\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "Now, we can see all three viewing planes for this 3-dimensional brain scan!\n",
    "\n",
    "## Exercises\n",
    "\n",
    "#### End of chapter Exercises\n",
    "\n",
    "**Assignment**\n",
    "Using the image from the the beginning of this lesson, \"rat_cerebellum.jpg\", do the following tasks:\n",
    "\n",
    "1. Import the image and display it.\n",
    "\n",
    "2. Show histograms of each of the colour channels and plot the contributions of each of the RGB colours separately.\n",
    "\n",
    "3. Create three different binary masks using manually determined thresholds: one for mostly red pixels, one for mostly green pixels, and one for mostly blue pixels. Note that you can apply conditions that are either greater than or smaller than a threshold of your choice.\n",
    "\n",
    "4. Plot the three masks and the corresponding masked images.\n",
    "\n",
    "5. Using your masks, approximate the relative amounts of synaptophysin, IP3R, and DNA in the image. To do this, you can assume that the number of red pixels represents synaptophysin, green pixels represents IP3R and blue pixels represent DNA. The results will vary depending on the setting of the thresholds. How do different theshold values change your results?\n",
    "\n",
    "6. Change the resolution of your image to different values. How does the resolution affect your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7848b5b7",
   "metadata": {},
   "source": [
    "## Please check these solutions only after submitting the assignments.\n",
    "\n",
    "### Q1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e1fd43",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## Import the image\n",
    "from matplotlib.image import imread\n",
    "\n",
    "img_task = imread('fig/rat_cerebellum.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the image\n",
    "\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "fig, ax = subplots(figsize=(20, 10))\n",
    "\n",
    "ax.imshow(img_task, cmap='gray');\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8b4ee",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-33-33.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "### Q2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_channel   = img_task[:, :, 0]\n",
    "green_channel = img_task[:, :, 1]\n",
    "blue_channel  = img_task[:, :, 2]\n",
    "\n",
    "fig, ax = subplots(ncols=3, figsize=(20, 5))\n",
    "\n",
    "ax[0].hist(red_channel.flatten(), bins=50)\n",
    "ax[0].set_xlabel(\"Pixel intensity\", fontsize=16)\n",
    "ax[0].set_xlabel(\"Red channel\")\n",
    "ax[1].hist(green_channel.flatten(), bins=50)\n",
    "ax[1].set_xlabel(\"Pixel intensity\", fontsize=16)\n",
    "ax[1].set_xlabel(\"Green channel\")\n",
    "ax[2].hist(blue_channel.flatten(), bins=50)\n",
    "ax[2].set_xlabel(\"Pixel intensity\", fontsize=16)\n",
    "ax[2].set_xlabel(\"Blue channel\");\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9bba1",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-34-35.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aef034",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(ncols=3, figsize=(20, 10))\n",
    "\n",
    "imgplot_red   = ax[0].imshow(red_channel, cmap=\"Reds\")\n",
    "imgplot_green = ax[1].imshow(green_channel, cmap=\"Greens\")\n",
    "imgplot_blue  = ax[2].imshow(blue_channel, cmap=\"Blues\")\n",
    "fig.colorbar(imgplot_red,   ax=ax[0], shrink=0.5)\n",
    "fig.colorbar(imgplot_green, ax=ax[1], shrink=0.5)\n",
    "fig.colorbar(imgplot_blue,  ax=ax[2], shrink=0.5);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c9e34",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-35-37.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "### Q3-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f34318",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_mask   = red_channel   > 120\n",
    "green_mask = green_channel > 100\n",
    "blue_mask  = blue_channel  > 100\n",
    "\n",
    "red_masked   = red_channel*red_mask\n",
    "green_masked = green_channel*green_mask\n",
    "blue_masked  = blue_channel*blue_mask\n",
    "\n",
    "fig, ax = subplots(nrows=3, ncols=2, figsize=(18, 20))\n",
    "\n",
    "ax[0, 0].imshow(red_mask, cmap='gray')\n",
    "ax[0, 0].set_title('Red binary mask', fontsize=16)\n",
    "ax[0, 1].imshow(red_masked, cmap='Reds')\n",
    "ax[0, 1].set_title('Masked image', fontsize=16)\n",
    "ax[1, 0].imshow(green_mask, cmap='gray')\n",
    "ax[1, 0].set_title('Green binary mask', fontsize=16)\n",
    "ax[1, 1].imshow(green_masked, cmap='Greens')\n",
    "ax[1, 1].set_title('Masked image', fontsize=16)\n",
    "ax[2, 0].imshow(blue_mask, cmap='gray')\n",
    "ax[2, 0].set_title('Blue binary mask', fontsize=16)\n",
    "ax[2, 1].imshow(blue_masked, cmap='Blues')\n",
    "ax[2, 1].set_title('Masked image', fontsize=16);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc4ccd",
   "metadata": {},
   "source": [
    "<img src=\"fig/03-image_handling-rendered-unnamed-chunk-36-39.png\" width=\"1728\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "### Q5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pixels = img_task.shape[0]*img_task.shape[1]\n",
    "\n",
    "red_counts   = sum(red_mask)\n",
    "green_counts = sum(green_mask)\n",
    "blue_counts  = sum(blue_mask)\n",
    "print(\"Approximately %d\"%(red_counts/total_pixels*100)+\"% of the image is synaptophysin\")\n",
    "Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: %d format: a real number is required, not numpy.ndarray\n",
    "\n",
    "Detailed traceback:\n",
    "  File \"<string>\", line 1, in <module>\n",
    "print(\"Approximately %d\"%(green_counts/total_pixels*100)+\"% of the image is IP3R\")\n",
    "Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: %d format: a real number is required, not numpy.ndarray\n",
    "\n",
    "Detailed traceback:\n",
    "  File \"<string>\", line 1, in <module>\n",
    "print(\"Approximately %d\"%(blue_counts/total_pixels*100)+\"% of the image is DNA\") \n",
    "Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: %d format: a real number is required, not numpy.ndarray\n",
    "\n",
    "Detailed traceback:\n",
    "  File \"<string>\", line 1, in <module>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f425b5",
   "metadata": {},
   "source": [
    "### Q6\n",
    "\n",
    "```\n",
    "[ad libitum]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f958f3a",
   "metadata": {},
   "source": [
    "- `imread` function can interpret many different image formats.\n",
    "- Masking isolates pixels whose intensity value is below a certain threshold.\n",
    "- The colour images are comprised of three channels (corresponding to red, green and blue intensities).\n",
    "- Python Image Library (PIL) helps to set high pixel limit for larger images.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
